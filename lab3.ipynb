{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Pytorch and Deep Networks\n",
    "\n",
    "Objectives:\n",
    "1. *Understand how to use Pytorch to write a Neural Network*\n",
    "2. *Write a neural network with multiple hidden layers*\n",
    "\n",
    "In the previous labs we saw how to write a Neural Network with one hidden layer to solve various problems. The main issue with writing a network from scratch is that it becomes quite tedious to work with multiple layers, especially when, in the training phase, we want to tweak our network. As an example, if we wanted to change the activation functions, the error functions or the optimiser (for instance using something different than classic gradient descent) that would imply rewriting the vast majority of the code.\n",
    "\n",
    "Luckily there are several libraries in Python that help us automate the process of writing a network in a modular way. The one we are going to explore in this module is Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch uses a specific data structure to handle data, called a \"tensor\". In the context of deep learning, a tensor is the generalisation of an array or a matrix in an arbitrary number of dimensions (for instance, a vector and a matrix would be a 1-dimensional and a 2-dimensional tensors respectively). The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor.\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/owKJBd8.png\" height=\"100px\"></center>\n",
    "\n",
    "A great feature of Pytorch is `Autograd`. PyTorch tensors keep track of the operations that involved them and they automatically provide the series of derivatives with respect to their inputs. This means you wonâ€™t need to write expressions for the backpropagation algorithm and compute derivatives by hand, as was done in previous labs. Pytorch, no matter how complicated your model is, will compute them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use is called MNIST and it contains a series of images of handwritten digits. The goal is to teach our network to properly classify the digits. \n",
    "\n",
    "Here is an example of the images that we are going to use:\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" height=\"250px\"></center>\n",
    "\n",
    "The database is organized as follows:\n",
    "- Input is a 28 by 28 matrix (2-dimensional tensor), which corresponds to the pixels of the image\n",
    "- The output is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the database\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Get data and store in variables \"trainset\" and \"testset\"\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to use the following architecture in order to properly classify our dataset:\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/dotjyD1.png\" height=\"250px\"></center>\n",
    "\n",
    "According to the picture above, we want a network that takes as an input a 784 (28*28 pixels) dimensional vector, have 3 hidden layers of 64 neurons each and an output layer of 10 units (the integers from 0 to 9). The activation functions should be sigmoid for the hidden layers and the softmax for the output layer. This is a really big network compared to the ones we used before and it would be quite complex to write it from scratch. Luckily, the code for a network such as this one is extremely simple to write in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class from nn.Module. This PyTorch module\n",
    "# contains the building blocks to create neural networks.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Defines the neural network architecure.\"\"\"\n",
    "        \n",
    "        super().__init__() # calls __init() of nn.Module superclass\n",
    "\n",
    "        # There are various functions that can be used to create different layers\n",
    "        #\n",
    "        # nn.Linear is the basic structure of layer where an affine transformation\n",
    "        # (preserves lines and parallelism) is applied to the input, i.e. w^T*x+b\n",
    "        #\n",
    "        # nn.Linear receives the input and output dimensions as arguments, and\n",
    "        # intialises a certain number of weights (tensors) accordingly\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 64) # create 64-neuron layer with 784 inputs each\n",
    "        self.fc2 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc3 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc4 = nn.Linear(64, 10)    # 64 inputs and 10 outputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output of the network for an input x.\"\"\"\n",
    "\n",
    "        # torch.sigmoid computes sigmoid function of given input\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # F.sofmax computes softmax function of given input\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net() # create instance of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finish this network we still need to set up the optimiser and error function, and compute the gradient with respect to the loss, then train for a number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimisation method to use in the backpropagation,\n",
    "# which in this case is stochastic gradient descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "# If zero_grad() isn't used, then at each iteration autograd will\n",
    "# keep adding the new value of the gradient to the previous one\n",
    "#\n",
    "# view() transforms 2D tensor input (28*28 matrix) to 1D (784 vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid, SGD\n",
      "Accuracy:  0.953\n"
     ]
    }
   ],
   "source": [
    "# Test if training was successful\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # no_grad means network won't update gradient stored in each\n",
    "    # variable during test session as there is no loss to be minimised\n",
    "    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('Sigmoid, SGD')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix = [[0,0],[0,0]]\n",
    "acc_matrix[0][0] = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the low accuracy, this doesn't work. The main problem here is that we are not training for enough epochs, and that stochastic gradient descent is generally very slow.\n",
    "\n",
    "Now try to play with the network a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid, Adam\n",
      "Accuracy:  0.957\n"
     ]
    }
   ],
   "source": [
    "# Changing optimissr from SGD to Adam\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('Sigmoid, Adam')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[0][1] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLu, SGD\n",
      "Accuracy:  0.114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7096/2476848637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# working with classifier, so using cross-entropy for loss func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# computes gradient wrt loss function over each network param\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# updates network paramaters (according to optimisation algo) and grad stored in each variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Change the sigmoid function with a ReLu function\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Defines the neural network architecure.\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # create 64-neuron layer with 784 inputs each\n",
    "        self.fc2 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc3 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc4 = nn.Linear(64, 10)    # 64 inputs and 10 outputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output of the network for an input x.\"\"\"\n",
    "\n",
    "        # F.relu applies the rectified linear function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # F.sofmax computes softmax function of given input\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net() # create instance of neural network\n",
    "\n",
    "\n",
    "# SGD optimiser\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('ReLu, SGD')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[1][0] = accuracy\n",
    "\n",
    "# Adam optimiser\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('ReLu, Adam')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[1][1] = accuracy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff39433d367775670857b5a26fe0f62904f7bab0aa1764437f86463c08ac313f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
