{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 -- Neural Networks Basics\n",
    "Learn the basics of neural networks and their underlying principles.\n",
    "\n",
    "Objectives:\n",
    "1. *Understand and code a simple neuron*\n",
    "2. *Understand how a neuron learns*\n",
    "3. *Understand its limitations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the library Numpy.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let‚Äôs code our first neuron. The structure of a neuron is as follows:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1302/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg\" height=\"300\">\n",
    "\n",
    "The function $ùëì(\\cdot)$ represents the core of the neuron. For the time being, let‚Äôs consider the sigmoid function:\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-(\\vec{w}^T\\vec{x}+b)}}$$\n",
    "where $\\vec{w} = (w_1,w_2,...,w_n)$ and $\\vec{x} = (x_1,x_2,...,x_n)$ represent the $n$-dimensional vectors for the weights and inputs, $b$ is the bias and $(\\cdot)^T$ is the transpose operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code required to simulate a neuron with ùëõ = 2, a sigmoid activation function and random weights.\n",
    "\n",
    "W = np.random.randn(1,2) # 1-by-2 array\n",
    "B = np.random.randn(1)\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    M = 1/(1+np.exp(-(X.dot(W.T)+B)))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs briefly recall the logical operators. Logical or Boolean, functions are defined as\n",
    "$g:{0,1}^n \\rightarrow {0,1}$. A two dimensional (i.e., $n=2$) logical function would have the form $g(x_1,x_2)=y$ with $x_1,x_2,y \\in {0,1}$.\n",
    "\n",
    "By this point our neuron is not yet able to learn. In order to make our unit smarter, we are going to make use of the gradient descent method to update the weights, depending on the choice of an error function. For this laboratory, we will consider the squared error function.\n",
    "\n",
    "$$E = (\\hat{y}-o)^2 \\\\\n",
    "w_i^\\prime = w_i + \\eta\\frac{dE}{dw_i} \\\\\n",
    "b^\\prime = b+\\eta\\frac{dE}{db}\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the correct output, $o$ is the current output of the neuron and $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** derive an analytical expression for the formulas above, considering that the derivative the sigmoid can be expressed as\n",
    "\n",
    "$$\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff39433d367775670857b5a26fe0f62904f7bab0aa1764437f86463c08ac313f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
