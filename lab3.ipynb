{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Pytorch and Deep Networks\n",
    "\n",
    "Objectives:\n",
    "1. *Understand how to use Pytorch to write a Neural Network*\n",
    "2. *Write a neural network with multiple hidden layers*\n",
    "\n",
    "In the previous labs we saw how to write a Neural Network with one hidden layer to solve various problems. The main issue with writing a network from scratch is that it becomes quite tedious to work with multiple layers, especially when, in the training phase, we want to tweak our network. As an example, if we wanted to change the activation functions, the error functions or the optimiser (for instance using something different than classic gradient descent) that would imply rewriting the vast majority of the code.\n",
    "\n",
    "Luckily there are several libraries in Python that help us automate the process of writing a network in a modular way. The one we are going to explore in this module is Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch uses a specific data structure to handle data, called a \"tensor\". In the context of deep learning, a tensor is the generalisation of an array or a matrix in an arbitrary number of dimensions (for instance, a vector and a matrix would be a 1-dimensional and a 2-dimensional tensors respectively). The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor.\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/owKJBd8.png\" height=\"100px\"></center>\n",
    "\n",
    "A great feature of Pytorch is `Autograd`. PyTorch tensors keep track of the operations that involved them and they automatically provide the series of derivatives with respect to their inputs. This means you won’t need to write expressions for the backpropagation algorithm and compute derivatives by hand, as was done in previous labs. Pytorch, no matter how complicated your model is, will compute them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use is called MNIST and it contains a series of images of handwritten digits. The goal is to teach our network to properly classify the digits. \n",
    "\n",
    "Here is an example of the images that we are going to use:\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" height=\"250px\"></center>\n",
    "\n",
    "The database is organized as follows:\n",
    "- Input is a 28 by 28 matrix (2-dimensional tensor), which corresponds to the pixels of the image\n",
    "- The output is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the database\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Get data and store in variables \"trainset\" and \"testset\"\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to use the following architecture in order to properly classify our dataset:\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/dotjyD1.png\" height=\"250px\"></center>\n",
    "\n",
    "According to the picture above, we want a network that takes as an input a 784 (28*28 pixels) dimensional vector, have 3 hidden layers of 64 neurons each and an output layer of 10 units (the integers from 0 to 9). The activation functions should be sigmoid for the hidden layers and the softmax for the output layer. This is a really big network compared to the ones we used before and it would be quite complex to write it from scratch. Luckily, the code for a network such as this one is extremely simple to write in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class from nn.Module. This PyTorch module\n",
    "# contains the building blocks to create neural networks.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Defines the neural network architecure.\"\"\"\n",
    "        \n",
    "        super().__init__() # calls __init() of nn.Module superclass\n",
    "\n",
    "        # There are various functions that can be used to create different layers\n",
    "        #\n",
    "        # nn.Linear is the basic structure of layer where an affine transformation\n",
    "        # (preserves lines and parallelism) is applied to the input, i.e. w^T*x+b\n",
    "        #\n",
    "        # nn.Linear receives the input and output dimensions as arguments, and\n",
    "        # intialises a certain number of weights (tensors) accordingly\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 64) # create 64-neuron layer with 784 inputs each\n",
    "        self.fc2 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc3 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc4 = nn.Linear(64, 10)    # 64 inputs and 10 outputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output of the network for an input x.\"\"\"\n",
    "\n",
    "        # torch.sigmoid computes sigmoid function of given input\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # F.sofmax computes softmax function of given input\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net() # create instance of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finish this network we still need to set up the optimiser and error function, and compute the gradient with respect to the loss, then train for a number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimisation method to use in the backpropagation,\n",
    "# which in this case is stochastic gradient descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "# If zero_grad() isn't used, then at each iteration autograd will\n",
    "# keep adding the new value of the gradient to the previous one\n",
    "#\n",
    "# view() transforms 2D tensor input (28*28 matrix) to 1D (784 vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid, SGD\n",
      "Accuracy:  0.101\n"
     ]
    }
   ],
   "source": [
    "# Test if training was successful\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # no_grad means network won't update gradient stored in each\n",
    "    # variable during test session as there is no loss to be minimised\n",
    "    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('Sigmoid, SGD')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix = [[0,0],[0,0]]\n",
    "acc_matrix[0][0] = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the low accuracy, this doesn't work. The main problem here is that we are not training for enough epochs, and that stochastic gradient descent is generally very slow.\n",
    "\n",
    "Now try to play with the network a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid, Adam\n",
      "Accuracy:  0.926\n"
     ]
    }
   ],
   "source": [
    "# Changing optimissr from SGD to Adam\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('Sigmoid, Adam')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[0][1] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLu, SGD\n",
      "Accuracy:  0.111\n",
      "ReLu, Adam\n",
      "Accuracy:  0.936\n"
     ]
    }
   ],
   "source": [
    "# Change the sigmoid function with a ReLu function\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Defines the neural network architecure.\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # create 64-neuron layer with 784 inputs each\n",
    "        self.fc2 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc3 = nn.Linear(64, 64)    # 64 inputs and 64 outputs\n",
    "        self.fc4 = nn.Linear(64, 10)    # 64 inputs and 10 outputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output of the network for an input x.\"\"\"\n",
    "\n",
    "        # F.relu applies the rectified linear function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # F.sofmax computes softmax function of given input\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net() # create instance of neural network\n",
    "\n",
    "\n",
    "# SGD optimiser\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('ReLu, SGD')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[1][0] = accuracy\n",
    "\n",
    "# Adam optimiser\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()  # sets up gradient stored in each variable of the network to zero\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y) # working with classifier, so using cross-entropy for loss func\n",
    "        loss.backward()  # computes gradient wrt loss function over each network param\n",
    "        optimizer.step() # updates network paramaters (according to optimisation algo) and grad stored in each variable\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = round(correct/total, 3)\n",
    "print('ReLu, Adam')\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "acc_matrix[1][1] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Performance/Accuracy\n",
      "          SGD   Adam\n",
      "Sigmoid [0.101, 0.926]\n",
      "   ReLU [0.111, 0.936]\n"
     ]
    }
   ],
   "source": [
    "# Summary of network performance/accuracies\n",
    "print('Network Performance/Accuracy')\n",
    "print('          SGD   Adam')     # different optimisers\n",
    "print(f'Sigmoid {acc_matrix[0]}') # sigmoid activation function\n",
    "print(f'   ReLU {acc_matrix[1]}') # ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can you see a significant change in the performance of the network? Why do you think that’s the case?*\n",
    "\n",
    "There is slight increase in accuracy when the activation functions is changed from sigmoid to ReLU. This advantage comes from the  derivative of the sigmoid function being very close to zero for small and large values. When calculating a weight's error derivative at the start of the network, a lot of sigmoid derivatives are multiplied with each (one for each layer), leading to \"gradient-vanishing\", where weights practically not being updated. Using the ReLU (whose gradient is either zero or one) would solves this problem entirely, the effect of which would be more apparent if the number of network layers is increased.\n",
    "\n",
    "On the other hand, the performance impact is much more signifcant when the optimiser is changed from SGD to the Adam. SGD, while very simple to understand and useful in many scenarios, fails to train more complex networks due to the presence of too many local minima. In addition, SGD is notorious for having poor numerical performance in \"ill-conditioned problems\", where a small change in the input can have a large change in function value, i.e. errors will be magnified. The Adam algorithm, however, has a much better affinity to solving large problems, which this handwriting classification scenario is."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff39433d367775670857b5a26fe0f62904f7bab0aa1764437f86463c08ac313f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
